from raid6_code import RSCode
import os
import string
import math
from array import array
from struct import pack

hsep = '|'
data_dir = 'data'
drives_dir = 'drives'
drive_prefix = 'drive_'


def encode_file(fname, part_count=6, parity_count=2, part_size=1024):
    # get input file and its info
    in_file_name = os.path.join(os.getcwd(), data_dir, fname)
    in_file = open(in_file_name, 'rb')
    in_size = get_file_size(in_file_name)

    block_size = part_size * part_count

    # setup storage drives
    drive_count = part_count + parity_count
    if drive_count > 256 or part_count <= 0:
        raise Exception('Invalid (drive_count,part_count), need 0 < part_count < drive_count < 257.')

    drives = init_drives(drive_count)

    header = make_header(fname, in_size, drive_count, part_count, block_size)

    code = RSCode(drive_count, part_count)

    for i in range(int(math.ceil(float(in_size) / block_size))):
        # out_parts are the partitions and parities generated by one block,
        # there are drive_count number of out_part:
        # out_parts[0 ~ part_count-1] are partitions separated from the block
        # out_parts[part_count ~ drive_count-1] are the parity_count number of parities
        out_parts = [file]*drive_count

        # since we need to distribute the parities across the drives in a cyclist manner,
        # parity_index is calculated to indicate the starting drive for the parity part
        # so drive[0] to drive[parity_index-1] stores the partitions
        # drive[parity_index] to drive[parity_index + parity_count - 1] stores the parity_count number of parities
        # drive[parity_index + parity_count] to drive[drive_count-1] stores the rest of the partitions
        #
        # for the header of each part, the partition parts stores the partition index (i.e. the ith part of the block)
        # the parity parts stores the parity index(the ith parity)
        parity_index = part_count - i % (part_count+1)
        for j in range(parity_index):
            out_part_name = os.path.join(drives[j], fname + '.pt' + repr(i))
            out_parts[j] = open(out_part_name, 'wb')
            out_parts[j].write(header + repr(j) + '\n')

        for j in range(parity_index, (parity_index + parity_count)):
            out_part_name = os.path.join(drives[j], fname + '.p' + repr(i))
            out_parts[part_count + j-parity_index] = open(out_part_name, 'wb')
            out_parts[part_count + j-parity_index].write(header + 'p' + repr(j-parity_index) + '\n')

        for j in range((parity_index + parity_count), drive_count):
            out_part_name = os.path.join(drives[j], fname + '.pt' + repr(i))
            out_parts[j-parity_count] = open(out_part_name, 'wb')
            out_parts[j-parity_count].write(header + repr(j-parity_count) + '\n')

        # calculate current block size
        # this is to cater for arbitrary file size other than multiplication of block_size
        if i < in_size / block_size:
            file_size = block_size
        else:
            file_size = in_size % block_size

        # encode file by field with field size == part_count
        for j in range(0, (file_size / part_count) * part_count, part_count):
            read_encode_and_write_block(part_count, in_file, out_parts, code)
        if file_size % part_count > 0:
            read_encode_and_write_block(file_size % part_count, in_file, out_parts, code)

        for j in range(drive_count):
            out_parts[j].close()

    print 'done!'


def decode_file(fname, out_name):
    out_file = open(os.path.join(os.getcwd(), data_dir, out_name), 'wb')

    drives = get_drives()
    (drive_count, part_count, file_size, block_size) = get_code_info(drives, fname)
    (recovered_drives, missing_drives, drive_mapper) = detect_and_rebuild_drives(drives, drive_count)

    # generate the code
    code = RSCode(drive_count, part_count)
    # start to decode
    recovered_size = 0
    i = 0
    while recovered_size < file_size:
        print 'trying to recover part ' + repr(i+1) + \
                                 ' of ' + repr(int(math.ceil(float(file_size)/block_size))) + '...'
        (block_file_list, dec_list, missed_list) = get_uncorrupted_parts(fname, drives, drive_count, part_count, i)
        code.prepare_decoder(dec_list[0:min(len(dec_list), part_count)])

        # initialize files to be recovered
        missed_parts = [None]*(len(missed_list))
        for j in range(len(missed_list)):
            header = make_header(fname, file_size, drive_count, part_count, block_size)
            missed_parts[j] = create_recover_part(fname, missed_list[j], drive_count, part_count,
                                                  i, drive_mapper, header)

        # decode and write to out_file
        if recovered_size + block_size <= file_size:
            decode_size = block_size
        else:
            decode_size = file_size % block_size

        for j in range(0, (decode_size / part_count) * part_count, part_count):
            read_decode_and_write_block(part_count, block_file_list[0:min(len(dec_list), part_count)],
                                        out_file, code,
                                        missed_parts, missed_list)
        if decode_size % part_count > 0:
            read_decode_and_write_block(decode_size % part_count, block_file_list[0:min(len(dec_list), part_count)],
                                        out_file, code,
                                        missed_parts, missed_list)

        for j in range(len(missed_list)):
            missed_parts[j].close()
        i += 1
        recovered_size += block_size
        print 'done!'
        print ''

    if len(missing_drives) > 0:
        var = raw_input("Do you want to rename the replacement storage back to failed storage name? (Y/N): ")
        if var == 'Y':
            for i in range(len(missing_drives)):
                os.rename(recovered_drives[i], os.path.join(os.getcwd(), drives_dir, "drive_" + repr(missing_drives[i])))
            print "done!"

    return 0


def test(original_name, recovered_name):
    original = open(os.path.join(os.getcwd(), data_dir, original_name), 'rb')
    recovered = open(os.path.join(os.getcwd(), data_dir, recovered_name), 'rb')
    match = (original.read() == recovered.read())
    return match


def get_file_size(fname):
    return os.stat(fname)[6]


def make_header(fname, size, drive_count, part_count, block_size):
    return string.join(['RS_PARITY_PIECE_HEADER', 'FILE', fname,
                        'drive_no', repr(drive_count), 'part_no', repr(part_count),
                        'size', repr(size), 'block_size', repr(block_size),
                        'piece'],
                       hsep) + hsep


def parse_header(header):
    return string.split(header, hsep)


# try get the infos from one of the available files
def get_code_info(drives, fname):
    for i in range(len(drives)):
        list_files = os.listdir(drives[i])
        for j in range(len(list_files)):
            if list_files[j].find(fname + '.') == 0:                 # find fname's parts
                file_path = os.path.join(drives[i], list_files[j])
                if os.path.isfile(file_path):
                    file = open(file_path, 'rb')
                    header = file.readline()
                    paras = parse_header(header)
                    file.close()
                    return int(paras[4]), int(paras[6]), int(long(paras[8])), int(long(paras[10]))
    assert False


def init_drives(drive_count):
    # setup folder paths
    prefix = os.path.join(os.getcwd(), drives_dir, drive_prefix)
    drives = [str]*drive_count
    # setup drive folder paths and create the drive folders if necessary
    for i in range(drive_count):
        drives[i] = prefix + repr(i)
        if not os.path.exists(drives[i]):
            os.makedirs(drives[i])
    return drives


# scan for drives
def get_drives():
    drives = [str]*256
    drives_count = 0
    listdir = os.listdir(os.path.join(os.getcwd(), drives_dir))
    for i in range(len(listdir)):
        drive_path = os.path.join(os.getcwd(), drives_dir, listdir[i])
        if os.path.isdir(drive_path):
            drives[drives_count] = drive_path
            drives_count += 1

    print 'Storage nodes available:'
    for i in range(drives_count):
        print '    ' + os.path.relpath(drives[i])
    print ''
    print ''
    return drives[0:drives_count]


# rebuild the failure drives, assuming the drives are drive_0, drive_1, drive_2, .... drive_{drive_count-1} previously
# then create replacement drives: drive_{drive_count}, drive_{drive_count+1}
def detect_and_rebuild_drives(drives, drive_count):
    recovered_drives = [str]*(drive_count-len(drives))
    missing_drives = range(drive_count)
    for i in range(len(drives)):
        number = int(drives[i][drives[i].find('_')+1:])
        missing_drives.remove(number)
    assert len(missing_drives) == drive_count-len(drives)
    print 'missing drive: ' + repr(missing_drives)

    print 'create replacement storage node'
    drive_mapper = range(drive_count)
    if len(missing_drives) > 0:
        for i in range(len(missing_drives)):
            recovered_drives[i] = os.path.join(os.getcwd(), drives_dir, "drive_") + repr(drive_count+i)
            os.makedirs(recovered_drives[i])
            drive_mapper[missing_drives[i]] = drive_count+i
    print 'done!'
    print ''
    print ''
    return recovered_drives, missing_drives, drive_mapper


# all the uncorrupted parts will be stored in the part_list, regardless the actual part_index (could be messed up)
# the actual part_index is decoded from the header file of the part and stored in the dec_list
# recall the part_index: 0~part_count-1 are the partitions and part_count~drive_count-1 are the parities
# missed_list stores the part_index that possibly corrupted, it will be used for recovering these missing parts
def get_uncorrupted_parts(fname, drives, drive_count, part_count, block_no):
    part_list = [str]*len(drives)
    dec_list = [int]*len(drives)
    missed_list = range(drive_count)
    un_corrupted_count = 0
    for i in range(len(drives)):
        # flexible way to find the files, it only depends on what was wrote in header file
        part_file_name  = os.path.join(drives[i], fname) + '.pt' + repr(block_no)
        parity_name = os.path.join(drives[i], fname) + '.p' + repr(block_no)
        if os.path.exists(part_file_name):
            part_name = part_file_name
        elif os.path.exists(parity_name):
            part_name = parity_name
        else:
            continue

        part_file = open(part_name, 'rb')
        header = part_file.readline()
        part_index = parse_header(header)[12]               # part_index is the index describing whether the part is a
                                                            # partition or parity
        part_list[un_corrupted_count] = part_file
        if part_index.find('p') == 0:                       # parity
            dec_list[un_corrupted_count] = part_count + int(part_index[1:])
        else:                                               # partition
            dec_list[un_corrupted_count] = int(part_index)
        missed_list.remove(dec_list[un_corrupted_count])
        un_corrupted_count += 1

    if len(part_list) < part_count:
        print 'unable to recovery part: ' + repr(block_no+1) + ', too many parts are missing'
        return -1
    else:
        print 'found uncorrupted parts:'
        list = '    '
        for s in range(len(dec_list)):
            if dec_list[s] < part_count:
                list += repr(dec_list[s])
            else:
                list += 'p' + repr(dec_list[s] - part_count)
            list += ', '
        print list

    return part_list[0:un_corrupted_count], dec_list[0:un_corrupted_count], missed_list


# initialize files to be recovered
# missed_part_no is the part_index of the part to be recovered
# drive_mapper will be used to map the corrupted drive into newly recovered drive
# other parameters is used to create file name and header files similar with encoding process
def create_recover_part(fname, missed_part_no, drive_count, part_count, block_index, drive_mapper, header):
    parity_index = part_count - block_index % (part_count+1)      # parity block index
    if missed_part_no < part_count:    # it is one of the partitions
        drive_index = missed_part_no if missed_part_no < parity_index else missed_part_no + drive_count - part_count
        part = open(os.path.join(os.getcwd(), drives_dir,                       # drives directory
                                 "drive_"+repr(drive_mapper[drive_index]),      # drive name
                                 fname + '.pt' + repr(block_index)),            # file name
                    'wb')
        part.write(header + repr(missed_part_no) + '\n')
    else:
        drive_index = missed_part_no + parity_index - part_count
        part = open(os.path.join(os.getcwd(), drives_dir,                       # drives directory
                                 "drive_"+repr(drive_mapper[drive_index]),      # drives name
                                 fname + '.p' + repr(block_index)),             # file name
                    'wb')
        part.write(header + 'p' + repr(missed_part_no - part_count) + '\n')
    return part


def read_encode_and_write_block(read_size, in_file, out_files, code):
    buffer = array('B')
    buffer.fromfile(in_file, read_size)

    for i in range(read_size, code.k):  # if read_size is lesser than code.k, fill up with 0s
        buffer.append(0)

    code_vec = code.encode(buffer)
    for j in range(code.n):
        out_files[j].write(pack('B', code_vec[j]))


# dec_list and part_count is used to initialize the decoder
# missed_parts and missed_list is used to recover the possible corrupted files
def read_decode_and_write_block(write_size, in_parts, out_file, code, missed_parts, missed_list):
    buffer = array('B')
    for i in range(code.k):
        buffer.fromfile(in_parts[i], 1)
    result = code.decode(buffer.tolist())
    for i in range(write_size):
        out_file.write(pack('B', result[i]))

    if len(missed_list) > 0:
        code_vec = code.encode(result)
        for i in range(len(missed_list)):
            missed_parts[i].write(pack('B', code_vec[missed_list[i]]))
